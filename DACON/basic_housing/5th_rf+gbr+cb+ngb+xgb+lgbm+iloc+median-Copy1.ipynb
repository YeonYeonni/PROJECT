{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87933900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdcb6b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../dataset/housing/train.csv', encoding = 'utf-8')\n",
    "test = pd.read_csv('../dataset/housing/test.csv', encoding = 'utf-8')\n",
    "submission = pd.read_csv('../dataset/housing/sample_submission.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a6e916d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.iloc[:, 1:]\n",
    "test = test.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87f8fd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train['Garage Yr Blt'] < 2050]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc028557",
   "metadata": {},
   "source": [
    "## Target Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6072ce47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column median Encoding\n",
    "cat_cols = ['Exter Qual', 'Kitchen Qual', 'Bsmt Qual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b36580eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in cat_cols :\n",
    "    ord_df = train.groupby(c).target.median().reset_index(name = f'ord_{c}')\n",
    "    train = pd.merge(train, ord_df, how = 'left')\n",
    "    test = pd.merge(test, ord_df, how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9e7a481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 column 삭제\n",
    "train.drop(cat_cols, axis = 1, inplace = True)\n",
    "test.drop(cat_cols, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d16cb923",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Exter Qual' 'Bsmt Qual' 'Kitchen Qual'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4948/2696713734.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 'Exter Qual', 'Bsmt Qual', 'Kitchen Qual' 변수는 Overall Qual로 보면 되기 때문에 삭제\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Exter Qual'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Bsmt Qual'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Kitchen Qual'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Exter Qual'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Bsmt Qual'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Kitchen Qual'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\python38\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\python38\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4904\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[1;36m1.0\u001b[0m     \u001b[1;36m0.8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4905\u001b[0m         \"\"\"\n\u001b[1;32m-> 4906\u001b[1;33m         return super().drop(\n\u001b[0m\u001b[0;32m   4907\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4908\u001b[0m             \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\python38\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4148\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4149\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4150\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4152\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\python38\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   4183\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4184\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4185\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4186\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\python38\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6015\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6017\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6018\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6019\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Exter Qual' 'Bsmt Qual' 'Kitchen Qual'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# 'Exter Qual', 'Bsmt Qual', 'Kitchen Qual' 변수는 Overall Qual로 보면 되기 때문에 삭제\n",
    "train=train.drop(['Exter Qual', 'Bsmt Qual', 'Kitchen Qual'], axis=1, inplace=False)\n",
    "test=test.drop(['Exter Qual', 'Bsmt Qual', 'Kitchen Qual'], axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d3116bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "로그 변환 전 타겟 왜도 = 1.7204669822790544 / 로그 변환 후 타겟 왜도 = 0.08225196452806845\n"
     ]
    }
   ],
   "source": [
    "print(f'로그 변환 전 타겟 왜도 = {train.target.skew()} / 로그 변환 후 타겟 왜도 = {np.log1p(train.target).skew()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "043cfdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop('target', axis = 1)\n",
    "y = np.log1p(train.target)\n",
    "\n",
    "target = test[X.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0bf7440",
   "metadata": {},
   "outputs": [],
   "source": [
    "target.fillna(target.mean(), inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eefa481",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7aecae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from ngboost import NGBRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e13116b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NMAE(true, pred) -> float:\n",
    "    mae = np.mean(np.abs(true - pred))\n",
    "    score = mae / np.mean(np.abs(true))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a125dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmae_score = make_scorer(NMAE, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f714fde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits = 10, random_state = 42, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa6aa31",
   "metadata": {},
   "source": [
    "## RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b0480ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 FOLD Training.....\n",
      "1 FOLD NMAE = 0.0936869815642399\n",
      "\n",
      "2 FOLD Training.....\n",
      "2 FOLD NMAE = 0.11372083162720076\n",
      "\n",
      "3 FOLD Training.....\n",
      "3 FOLD NMAE = 0.09443308083710712\n",
      "\n",
      "4 FOLD Training.....\n",
      "4 FOLD NMAE = 0.10951429437846574\n",
      "\n",
      "5 FOLD Training.....\n",
      "5 FOLD NMAE = 0.08987572925170044\n",
      "\n",
      "6 FOLD Training.....\n",
      "6 FOLD NMAE = 0.09997599529487447\n",
      "\n",
      "7 FOLD Training.....\n",
      "7 FOLD NMAE = 0.08966072470724444\n",
      "\n",
      "8 FOLD Training.....\n",
      "8 FOLD NMAE = 0.10569501920345054\n",
      "\n",
      "9 FOLD Training.....\n",
      "9 FOLD NMAE = 0.09540092022299637\n",
      "\n",
      "10 FOLD Training.....\n",
      "10 FOLD NMAE = 0.10049471546417325\n",
      "\n",
      "10FOLD Mean of NMAE = 0.0992458292551453 & std = 0.007792121941923879\n"
     ]
    }
   ],
   "source": [
    "rf_pred = np.zeros(target.shape[0])\n",
    "rf_val = []\n",
    "for n, (tr_idx, val_idx) in enumerate(kf.split(X, y)) :\n",
    "    print(f'{n + 1} FOLD Training.....')\n",
    "    tr_x, tr_y = X.iloc[tr_idx], y.iloc[tr_idx]\n",
    "    val_x, val_y = X.iloc[val_idx], np.expm1(y.iloc[val_idx])\n",
    "    \n",
    "    rf = RandomForestRegressor(random_state = 42, criterion = 'mae')\n",
    "    rf.fit(tr_x, tr_y)\n",
    "    \n",
    "    val_pred = np.expm1(rf.predict(val_x))\n",
    "    val_nmae = NMAE(val_y, val_pred)\n",
    "    rf_val.append(val_nmae)\n",
    "    print(f'{n + 1} FOLD NMAE = {val_nmae}\\n')\n",
    "    \n",
    "    fold_pred = rf.predict(target) / 10\n",
    "    rf_pred += fold_pred\n",
    "print(f'10FOLD Mean of NMAE = {np.mean(rf_val)} & std = {np.std(rf_val)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5932eb89",
   "metadata": {},
   "source": [
    "## GradientBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "deb36b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 FOLD Training.....\n",
      "1 FOLD NMAE = 0.09367372438745086\n",
      "\n",
      "2 FOLD Training.....\n",
      "2 FOLD NMAE = 0.10452949253790086\n",
      "\n",
      "3 FOLD Training.....\n",
      "3 FOLD NMAE = 0.0822404359570606\n",
      "\n",
      "4 FOLD Training.....\n",
      "4 FOLD NMAE = 0.10848179566764835\n",
      "\n",
      "5 FOLD Training.....\n",
      "5 FOLD NMAE = 0.09544021807976999\n",
      "\n",
      "6 FOLD Training.....\n",
      "6 FOLD NMAE = 0.09940533669629992\n",
      "\n",
      "7 FOLD Training.....\n",
      "7 FOLD NMAE = 0.08873019327524258\n",
      "\n",
      "8 FOLD Training.....\n",
      "8 FOLD NMAE = 0.10423773655894322\n",
      "\n",
      "9 FOLD Training.....\n",
      "9 FOLD NMAE = 0.10540905806087426\n",
      "\n",
      "10 FOLD Training.....\n",
      "10 FOLD NMAE = 0.0939495270409796\n",
      "\n",
      "10FOLD Mean of NMAE = 0.09760975182621703 & std = 0.007891302159289498\n"
     ]
    }
   ],
   "source": [
    "gbr_pred = np.zeros(target.shape[0])\n",
    "gbr_val = []\n",
    "for n, (tr_idx, val_idx) in enumerate(kf.split(X, y)) :\n",
    "    print(f'{n + 1} FOLD Training.....')\n",
    "    tr_x, tr_y = X.iloc[tr_idx], y.iloc[tr_idx]\n",
    "    val_x, val_y = X.iloc[val_idx], np.expm1(y.iloc[val_idx])\n",
    "    \n",
    "    gbr = GradientBoostingRegressor(random_state = 42, max_depth = 4, learning_rate = 0.05, n_estimators = 1000)\n",
    "    gbr.fit(tr_x, tr_y)\n",
    "    \n",
    "    val_pred = np.expm1(gbr.predict(val_x))\n",
    "    val_nmae = NMAE(val_y, val_pred)\n",
    "    gbr_val.append(val_nmae)\n",
    "    print(f'{n + 1} FOLD NMAE = {val_nmae}\\n')\n",
    "    \n",
    "    fold_pred = gbr.predict(target) / 10\n",
    "    gbr_pred += fold_pred\n",
    "print(f'10FOLD Mean of NMAE = {np.mean(gbr_val)} & std = {np.std(gbr_val)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ac509f",
   "metadata": {},
   "source": [
    "## CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71d2e1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 FOLD Training.....\n",
      "0:\tlearn: 0.2928192\ttest: 189506.6733447\tbest: 189506.6733447 (0)\ttotal: 139ms\tremaining: 6m 56s\n",
      "Stopped by overfitting detector  (750 iterations wait)\n",
      "\n",
      "bestTest = 189506.5933\n",
      "bestIteration = 182\n",
      "\n",
      "Shrink model to first 183 iterations.\n",
      "1 FOLD NMAE = 0.09777341102660744\n",
      "\n",
      "2 FOLD Training.....\n",
      "0:\tlearn: 0.2929645\ttest: 186991.7521093\tbest: 186991.7521093 (0)\ttotal: 1.18ms\tremaining: 3.55s\n",
      "1000:\tlearn: 0.0677636\ttest: 186991.7205303\tbest: 186991.7187119 (578)\ttotal: 735ms\tremaining: 1.47s\n",
      "Stopped by overfitting detector  (750 iterations wait)\n",
      "\n",
      "bestTest = 186991.7187\n",
      "bestIteration = 578\n",
      "\n",
      "Shrink model to first 579 iterations.\n",
      "2 FOLD NMAE = 0.10260214892787221\n",
      "\n",
      "3 FOLD Training.....\n",
      "0:\tlearn: 0.2919462\ttest: 176765.4793246\tbest: 176765.4793246 (0)\ttotal: 1.14ms\tremaining: 3.42s\n",
      "Stopped by overfitting detector  (750 iterations wait)\n",
      "\n",
      "bestTest = 176765.4658\n",
      "bestIteration = 66\n",
      "\n",
      "Shrink model to first 67 iterations.\n",
      "3 FOLD NMAE = 0.12234936741494892\n",
      "\n",
      "4 FOLD Training.....\n",
      "0:\tlearn: 0.2928466\ttest: 187437.9592515\tbest: 187437.9592515 (0)\ttotal: 1.1ms\tremaining: 3.32s\n",
      "1000:\tlearn: 0.0680655\ttest: 187437.9124453\tbest: 187437.9117464 (564)\ttotal: 743ms\tremaining: 1.48s\n",
      "Stopped by overfitting detector  (750 iterations wait)\n",
      "\n",
      "bestTest = 187437.9117\n",
      "bestIteration = 564\n",
      "\n",
      "Shrink model to first 565 iterations.\n",
      "4 FOLD NMAE = 0.10216496915878988\n",
      "\n",
      "5 FOLD Training.....\n",
      "0:\tlearn: 0.2888456\ttest: 192915.6544942\tbest: 192915.6544942 (0)\ttotal: 921us\tremaining: 2.76s\n",
      "Stopped by overfitting detector  (750 iterations wait)\n",
      "\n",
      "bestTest = 192915.6008\n",
      "bestIteration = 198\n",
      "\n",
      "Shrink model to first 199 iterations.\n",
      "5 FOLD NMAE = 0.10200876686264966\n",
      "\n",
      "6 FOLD Training.....\n",
      "0:\tlearn: 0.2898561\ttest: 188506.9523046\tbest: 188506.9523046 (0)\ttotal: 965us\tremaining: 2.9s\n",
      "Stopped by overfitting detector  (750 iterations wait)\n",
      "\n",
      "bestTest = 188506.8951\n",
      "bestIteration = 151\n",
      "\n",
      "Shrink model to first 152 iterations.\n",
      "6 FOLD NMAE = 0.0840527628178528\n",
      "\n",
      "7 FOLD Training.....\n",
      "0:\tlearn: 0.2932652\ttest: 194497.7718628\tbest: 194497.7718628 (0)\ttotal: 1.23ms\tremaining: 3.69s\n",
      "1000:\tlearn: 0.0669996\ttest: 194497.6730782\tbest: 194497.6720627 (479)\ttotal: 773ms\tremaining: 1.54s\n",
      "Stopped by overfitting detector  (750 iterations wait)\n",
      "\n",
      "bestTest = 194497.6721\n",
      "bestIteration = 479\n",
      "\n",
      "Shrink model to first 480 iterations.\n",
      "7 FOLD NMAE = 0.08901641687559725\n",
      "\n",
      "8 FOLD Training.....\n",
      "0:\tlearn: 0.2939647\ttest: 174503.5921733\tbest: 174503.5921733 (0)\ttotal: 1.26ms\tremaining: 3.79s\n",
      "Stopped by overfitting detector  (750 iterations wait)\n",
      "\n",
      "bestTest = 174503.5699\n",
      "bestIteration = 60\n",
      "\n",
      "Shrink model to first 61 iterations.\n",
      "8 FOLD NMAE = 0.1336428807901599\n",
      "\n",
      "9 FOLD Training.....\n",
      "0:\tlearn: 0.2913289\ttest: 181768.1424972\tbest: 181768.1424972 (0)\ttotal: 1.18ms\tremaining: 3.54s\n",
      "Stopped by overfitting detector  (750 iterations wait)\n",
      "\n",
      "bestTest = 181768.1017\n",
      "bestIteration = 92\n",
      "\n",
      "Shrink model to first 93 iterations.\n",
      "9 FOLD NMAE = 0.118032679306418\n",
      "\n",
      "10 FOLD Training.....\n",
      "0:\tlearn: 0.2900080\ttest: 190480.0213943\tbest: 190480.0213943 (0)\ttotal: 1.08ms\tremaining: 3.25s\n",
      "1000:\tlearn: 0.0669407\ttest: 190479.9406481\tbest: 190479.9405925 (994)\ttotal: 792ms\tremaining: 1.58s\n",
      "Stopped by overfitting detector  (750 iterations wait)\n",
      "\n",
      "bestTest = 190479.9405\n",
      "bestIteration = 1105\n",
      "\n",
      "Shrink model to first 1106 iterations.\n",
      "10 FOLD NMAE = 0.1006012488909953\n",
      "\n",
      "10FOLD Mean of NMAE = 0.10522446520718913 & std = 0.014419885204659588\n"
     ]
    }
   ],
   "source": [
    "cb_pred = np.zeros(target.shape[0])\n",
    "cb_val = []\n",
    "for n, (tr_idx, val_idx) in enumerate(kf.split(X, y)) :\n",
    "    print(f'{n + 1} FOLD Training.....')\n",
    "    tr_x, tr_y = X.iloc[tr_idx], y.iloc[tr_idx]\n",
    "    val_x, val_y = X.iloc[val_idx], np.expm1(y.iloc[val_idx])\n",
    "    \n",
    "    tr_data = Pool(data = tr_x, label = tr_y)\n",
    "    val_data = Pool(data = val_x, label = val_y)\n",
    "    \n",
    "    cb = CatBoostRegressor(depth = 4, random_state = 42, loss_function = 'MAE', n_estimators = 3000, learning_rate = 0.03, verbose = 0)\n",
    "    cb.fit(tr_data, eval_set = val_data, early_stopping_rounds = 750, verbose = 1000)\n",
    "    \n",
    "    val_pred = np.expm1(cb.predict(val_x))\n",
    "    val_nmae = NMAE(val_y, val_pred)\n",
    "    cb_val.append(val_nmae)\n",
    "    print(f'{n + 1} FOLD NMAE = {val_nmae}\\n')\n",
    "    \n",
    "    target_data = Pool(data = target, label = None)\n",
    "    fold_pred = cb.predict(target) / 10\n",
    "    cb_pred += fold_pred\n",
    "print(f'10FOLD Mean of NMAE = {np.mean(cb_val)} & std = {np.std(cb_val)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515daa68",
   "metadata": {},
   "source": [
    "## NGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19077579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 FOLD Training.....\n",
      "1 FOLD NMAE = 0.09141854198095778\n",
      "\n",
      "2 FOLD Training.....\n",
      "2 FOLD NMAE = 0.10453644003185501\n",
      "\n",
      "3 FOLD Training.....\n",
      "3 FOLD NMAE = 0.09012342916688346\n",
      "\n",
      "4 FOLD Training.....\n",
      "4 FOLD NMAE = 0.10385877081603535\n",
      "\n",
      "5 FOLD Training.....\n",
      "5 FOLD NMAE = 0.08823461666867576\n",
      "\n",
      "6 FOLD Training.....\n",
      "6 FOLD NMAE = 0.0992715689789523\n",
      "\n",
      "7 FOLD Training.....\n",
      "7 FOLD NMAE = 0.09022827054658515\n",
      "\n",
      "8 FOLD Training.....\n",
      "8 FOLD NMAE = 0.09899957912118629\n",
      "\n",
      "9 FOLD Training.....\n",
      "9 FOLD NMAE = 0.10036670901025278\n",
      "\n",
      "10 FOLD Training.....\n",
      "10 FOLD NMAE = 0.09584672894941221\n",
      "\n",
      "10FOLD Mean of NMAE = 0.09628846552707962 & std = 0.005672117092757084\n"
     ]
    }
   ],
   "source": [
    "ngb_pred = np.zeros(target.shape[0])\n",
    "ngb_val = []\n",
    "for n, (tr_idx, val_idx) in enumerate(kf.split(X, y)) :\n",
    "    print(f'{n + 1} FOLD Training.....')\n",
    "    tr_x, tr_y = X.iloc[tr_idx], y.iloc[tr_idx]\n",
    "    val_x, val_y = X.iloc[val_idx], np.expm1(y.iloc[val_idx])\n",
    "    \n",
    "    ngb = NGBRegressor(random_state = 42, n_estimators = 1000, verbose = 0, learning_rate = 0.03)\n",
    "    ngb.fit(tr_x, tr_y, val_x, val_y, early_stopping_rounds = 300)\n",
    "    \n",
    "    val_pred = np.expm1(ngb.predict(val_x))\n",
    "    val_nmae = NMAE(val_y, val_pred)\n",
    "    ngb_val.append(val_nmae)\n",
    "    print(f'{n + 1} FOLD NMAE = {val_nmae}\\n')\n",
    "    \n",
    "    target_data = Pool(data = target, label = None)\n",
    "    fold_pred = ngb.predict(target) / 10\n",
    "    ngb_pred += fold_pred\n",
    "print(f'10FOLD Mean of NMAE = {np.mean(ngb_val)} & std = {np.std(ngb_val)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb1958e",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d953559b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 FOLD Training.....\n",
      "[16:36:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "1 FOLD NMAE = 0.10174294309067064\n",
      "\n",
      "2 FOLD Training.....\n",
      "[16:36:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "2 FOLD NMAE = 0.11585616326848466\n",
      "\n",
      "3 FOLD Training.....\n",
      "[16:36:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "3 FOLD NMAE = 0.09459694787723735\n",
      "\n",
      "4 FOLD Training.....\n",
      "[16:36:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "4 FOLD NMAE = 0.11035140709588621\n",
      "\n",
      "5 FOLD Training.....\n",
      "[16:36:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "5 FOLD NMAE = 0.10139366514739334\n",
      "\n",
      "6 FOLD Training.....\n",
      "[16:36:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "6 FOLD NMAE = 0.10581594894679221\n",
      "\n",
      "7 FOLD Training.....\n",
      "[16:36:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "7 FOLD NMAE = 0.09568942685012401\n",
      "\n",
      "8 FOLD Training.....\n",
      "[16:36:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "8 FOLD NMAE = 0.10478096014876226\n",
      "\n",
      "9 FOLD Training.....\n",
      "[16:36:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "9 FOLD NMAE = 0.10598642352522915\n",
      "\n",
      "10 FOLD Training.....\n",
      "[16:36:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "10 FOLD NMAE = 0.1101764248563412\n",
      "\n",
      "10FOLD Mean of NMAE = 0.1046390310806921 & std = 0.006262566140515817\n"
     ]
    }
   ],
   "source": [
    "# 초기화\n",
    "xgb_pred = np.zeros(target.shape[0])\n",
    "xgb_val = []\n",
    "\n",
    "for n, (tr_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "    print(f'{n + 1} FOLD Training.....')\n",
    "    tr_x, tr_y = X.iloc[tr_idx], y.iloc[tr_idx]\n",
    "    val_x, val_y = X.iloc[val_idx], np.expm1(y.iloc[val_idx])\n",
    "    \n",
    "    # xgb 학습\n",
    "    xgb = XGBRegressor(random_state = 42, criterion = 'mae')\n",
    "    xgb.fit(tr_x, tr_y)\n",
    "    \n",
    "    val_pred = np.expm1(xgb.predict(val_x))\n",
    "    val_nmae = NMAE(val_y, val_pred) # NMAE\n",
    "    xgb_val.append(val_nmae) # 초기화한 리스트에 NMAE를 삽입.\n",
    "    print(f'{n + 1} FOLD NMAE = {val_nmae}\\n')\n",
    "    \n",
    "    fold_pred = xgb.predict(target) / 10\n",
    "    xgb_pred += fold_pred\n",
    "\n",
    "print(f'10FOLD Mean of NMAE = {np.mean(xgb_val)} & std = {np.std(xgb_val)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fe58b4",
   "metadata": {},
   "source": [
    "## LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57bc88b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 FOLD Training.....\n",
      "[LightGBM] [Warning] Unknown parameter: criterion\n",
      "1 FOLD NMAE = 0.09201157240469633\n",
      "\n",
      "2 FOLD Training.....\n",
      "[LightGBM] [Warning] Unknown parameter: criterion\n",
      "2 FOLD NMAE = 0.10895172392077022\n",
      "\n",
      "3 FOLD Training.....\n",
      "[LightGBM] [Warning] Unknown parameter: criterion\n",
      "3 FOLD NMAE = 0.09168478976167703\n",
      "\n",
      "4 FOLD Training.....\n",
      "[LightGBM] [Warning] Unknown parameter: criterion\n",
      "4 FOLD NMAE = 0.11362699748623657\n",
      "\n",
      "5 FOLD Training.....\n",
      "[LightGBM] [Warning] Unknown parameter: criterion\n",
      "5 FOLD NMAE = 0.09112515275480314\n",
      "\n",
      "6 FOLD Training.....\n",
      "[LightGBM] [Warning] Unknown parameter: criterion\n",
      "6 FOLD NMAE = 0.10007852232252476\n",
      "\n",
      "7 FOLD Training.....\n",
      "[LightGBM] [Warning] Unknown parameter: criterion\n",
      "7 FOLD NMAE = 0.09450594667743578\n",
      "\n",
      "8 FOLD Training.....\n",
      "[LightGBM] [Warning] Unknown parameter: criterion\n",
      "8 FOLD NMAE = 0.10768739403591426\n",
      "\n",
      "9 FOLD Training.....\n",
      "[LightGBM] [Warning] Unknown parameter: criterion\n",
      "9 FOLD NMAE = 0.10647087418496351\n",
      "\n",
      "10 FOLD Training.....\n",
      "[LightGBM] [Warning] Unknown parameter: criterion\n",
      "10 FOLD NMAE = 0.10003430042614071\n",
      "\n",
      "10FOLD Mean of NMAE = 0.10061772739751622 & std = 0.007777489084609819\n"
     ]
    }
   ],
   "source": [
    "# 초기화\n",
    "lgbm_pred = np.zeros(target.shape[0])\n",
    "lgbm_val = []\n",
    "\n",
    "for n, (tr_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "    print(f'{n + 1} FOLD Training.....')\n",
    "    tr_x, tr_y = X.iloc[tr_idx], y.iloc[tr_idx]\n",
    "    val_x, val_y = X.iloc[val_idx], np.expm1(y.iloc[val_idx])\n",
    "    \n",
    "    # xgb 학습\n",
    "    lgbm = LGBMRegressor(random_state = 42, criterion = 'mae')\n",
    "    lgbm.fit(tr_x, tr_y)\n",
    "    \n",
    "    val_pred = np.expm1(lgbm.predict(val_x))\n",
    "    val_nmae = NMAE(val_y, val_pred) # NMAE\n",
    "    lgbm_val.append(val_nmae) # 초기화한 리스트에 NMAE를 삽입.\n",
    "    print(f'{n + 1} FOLD NMAE = {val_nmae}\\n')\n",
    "    \n",
    "    fold_pred = lgbm.predict(target) / 10\n",
    "    lgbm_pred += fold_pred\n",
    "\n",
    "print(f'10FOLD Mean of NMAE = {np.mean(lgbm_val)} & std = {np.std(lgbm_val)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0157ce75",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "199f92a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12.71889824, 11.75681089, 12.07424625, ..., 11.24701861,\n",
       "       12.19540471, 11.86582694])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(rf_pred + gbr_pred + cb_pred + ngb_pred + xgb_pred + lgbm_pred) / 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b96fa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = np.expm1((rf_pred + gbr_pred + cb_pred + ngb_pred + xgb_pred + lgbm_pred) / 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f43eafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('5th.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f3b633",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
